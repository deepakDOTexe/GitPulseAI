"""
GitLab Handbook Assistant - Streamlit App

This is a Streamlit-based chat interface for the GitLab Handbook Assistant.
It uses a hybrid RAG system with Hugging Face embeddings and Google Gemini LLM.
"""

import os
import streamlit as st
from datetime import datetime
import warnings
import logging
import sys
warnings.filterwarnings("ignore")

# Load environment variables
from dotenv import load_dotenv
load_dotenv()

# Configure logging
log_level = os.getenv("LOG_LEVEL", "INFO").upper()
log_format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
log_file = os.getenv("LOG_FILE", "logs/app.log")

# Ensure log directory exists
os.makedirs(os.path.dirname(log_file), exist_ok=True)

# Setup logging to both file and console
logging.basicConfig(
    level=getattr(logging, log_level),
    format=log_format,
    handlers=[
        logging.FileHandler(log_file),
        logging.StreamHandler(sys.stdout)
    ]
)

logger = logging.getLogger("GitPulseAI")

# 1. App title
st.set_page_config(
    page_title="üîçüí¨ GitPulseAI Chatbot",
    page_icon="üîç",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Detect deployment mode
USE_SUPABASE = os.getenv("USE_SUPABASE", "false").lower() == "true"
logger.info(f"Deployment mode: {'Cloud' if USE_SUPABASE else 'Local'}")
logger.debug(f"USE_SUPABASE environment variable: {os.getenv('USE_SUPABASE')}")

@st.cache_resource
def initialize_rag_system():
    """Initialize the appropriate RAG system based on environment."""
    try:
        logger.info("Initializing RAG system...")
        
        if USE_SUPABASE:
            # Use Supabase cloud RAG system
            logger.info("Using Supabase cloud RAG system")
            from src.supabase_rag_system import create_supabase_rag_system
            
            supabase_url = os.getenv("SUPABASE_URL")
            supabase_key = os.getenv("SUPABASE_KEY") 
            gemini_api_key = os.getenv("GEMINI_API_KEY")
            
            # Log configurations (masking sensitive values)
            logger.debug(f"SUPABASE_URL: {supabase_url[:10]}...{supabase_url[-5:] if supabase_url else None}")
            logger.debug(f"GEMINI_API_KEY: {'*' * 8}{gemini_api_key[-4:] if gemini_api_key else None}")
            logger.debug(f"SUPABASE_KEY: {'*' * 8}{supabase_key[-4:] if supabase_key else None}")
            
            if not supabase_url or not supabase_key or not gemini_api_key:
                error_msg = "‚ùå Missing required environment variables: "
                error_msg += "SUPABASE_URL, " if not supabase_url else ""
                error_msg += "SUPABASE_KEY, " if not supabase_key else ""
                error_msg += "GEMINI_API_KEY" if not gemini_api_key else ""
                logger.error(error_msg)
                st.error(error_msg)
                return None
            
            logger.info("Creating Supabase RAG system...")
            return create_supabase_rag_system(
                supabase_url=supabase_url,
                supabase_key=supabase_key,
                gemini_api_key=gemini_api_key
            )
        else:
            # Use local hybrid RAG system
            logger.info("Using local hybrid RAG system")
            data_file = os.getenv("SAMPLE_DATA_FILE")
            logger.info(f"Loading data from: {data_file}")
            
            from src.hybrid_rag_system import HybridRAGSystem
            return HybridRAGSystem()
            
    except Exception as e:
        error_msg = f"‚ùå Failed to initialize RAG system: {str(e)}"
        logger.exception(error_msg)
        st.error(error_msg)
        return None

# Initialize RAG system
rag_system = initialize_rag_system()
if not rag_system:
    st.error("Failed to initialize the RAG system. Please check your configuration.")
    st.stop()

# Sidebar
with st.sidebar:
    st.title('üîçüí¨ GitPulseAI')
    st.caption("GitLab Handbook Assistant")
    
    # Status with metrics
    status_text = "üå©Ô∏è Cloud Mode" if USE_SUPABASE else "üíª Local Mode"
    st.success(f'{status_text} - System Ready!', icon='‚úÖ')
    
    # System metrics
    if 'query_count' not in st.session_state:
        st.session_state.query_count = 0
    
    col1, col2 = st.columns(2)
    with col1:
        st.metric("Queries", st.session_state.query_count)
    with col2:
        st.metric("Mode", "Cloud" if USE_SUPABASE else "Local")
    
    st.divider()
    
    # Example questions in categories
    st.subheader('üí° Example Questions')
    
    with st.expander("üéØ Core Values & Culture"):
        questions_values = [
            "What are GitLab's core values?",
            "How does GitLab approach collaboration?",
            "What are GitLab's diversity initiatives?"
        ]
        for i, question in enumerate(questions_values):
            if st.button(question, key=f"val_{i}", use_container_width=True):
                st.session_state.messages.append({"role": "user", "content": question})
                st.session_state.query_count += 1
                st.rerun()
    
    with st.expander("üè† Remote Work & Policies"):
        questions_work = [
            "How does GitLab handle remote work?",
            "What is GitLab's anti-harassment policy?",
            "What are the company's results-focused principles?"
        ]
        for i, question in enumerate(questions_work):
            if st.button(question, key=f"work_{i}", use_container_width=True):
                st.session_state.messages.append({"role": "user", "content": question})
                st.session_state.query_count += 1
                st.rerun()
    
    st.divider()
    
    # Clear chat history button
    def clear_chat_history():
        st.session_state.messages = [{"role": "assistant", "content": "How may I help you explore GitLab's handbook today?"}]
        st.session_state.query_count = 0
        if 'last_sources' in st.session_state:
            del st.session_state.last_sources
    st.button('üóëÔ∏è Clear Chat History', on_click=clear_chat_history, use_container_width=True)
    
    # Last response sources
    if 'last_sources' in st.session_state and st.session_state.last_sources:
        st.divider()
        st.subheader('üìö Sources from Last Response')
        for i, source in enumerate(st.session_state.last_sources[:3]):  # Show top 3
            with st.expander(f"üìÑ {source.get('title', 'Unknown')[:30]}..."):
                if source.get('url'):
                    st.write(f"üîó [View Source]({source['url']})")
                if source.get('similarity_score'):
                    st.write(f"üìä Relevance: {source['similarity_score']:.1%}")

# 2. Store LLM generated responses
try:
    logger.info("Initializing chat session state")
    if "messages" not in st.session_state.keys():
        logger.info("Creating initial message in session state")
        st.session_state.messages = [{"role": "assistant", "content": "How may I help you explore GitLab's handbook today?"}]
    logger.info(f"Current session state has {len(st.session_state.messages)} messages")
except Exception as e:
    logger.exception("CRITICAL ERROR during session state initialization")
    st.error(f"Session initialization error: {str(e)}")
    import traceback
    st.code(traceback.format_exc())

# Welcome banner (only show when no chat history except initial message)
if len(st.session_state.messages) == 1:
    st.info(
        "üëã **Welcome to GitPulseAI!** Ask me anything about GitLab's handbook, policies, values, and culture. "
        "Use the example questions in the sidebar to get started, or type your own question below.",
        icon="üí¨"
    )

# 2. Display chat messages
try:
    logger.info("Starting chat message display")
    
    # Verify messages exist and have proper format
    if not isinstance(st.session_state.messages, list):
        logger.error(f"Messages is not a list: {type(st.session_state.messages)}")
        st.error("Invalid message format detected. Resetting chat.")
        st.session_state.messages = [{"role": "assistant", "content": "How may I help you explore GitLab's handbook today?"}]
    
    for i, message in enumerate(st.session_state.messages):
        logger.debug(f"Displaying message {i}: role={message.get('role', 'unknown')}, content_length={len(message.get('content', ''))}")
        
        # Verify message format
        if not isinstance(message, dict) or 'role' not in message or 'content' not in message:
            logger.warning(f"Skipping malformed message at index {i}: {message}")
            continue
            
        with st.chat_message(message["role"]):
            st.write(message["content"])
            
            # Add response feedback for assistant messages (except first welcome)
            if message["role"] == "assistant" and i > 0:
                try:
                    col1, col2, col3, col4 = st.columns([1, 1, 1, 6])
                    with col1:
                        if st.button("üëç", key=f"thumb_up_{i}", help="Helpful response"):
                            st.success("Thanks for your feedback!", icon="‚úÖ")
                    with col2:
                        if st.button("üëé", key=f"thumb_down_{i}", help="Not helpful"):
                            st.info("Thanks for your feedback! We'll improve.", icon="üìù")
                    with col3:
                        if st.button("üìã", key=f"copy_{i}", help="Copy response"):
                            st.code(message["content"], language=None)
                except Exception as e:
                    logger.exception(f"Error displaying feedback buttons for message {i}")
except Exception as e:
    logger.exception("CRITICAL ERROR while displaying chat messages")
    st.error("Error displaying chat messages. Please try refreshing the page.")
    import traceback
    st.code(traceback.format_exc())

# 3. Create the LLM response generation function
def generate_gitpulse_response(prompt_input):
    """Generate response using GitPulseAI RAG system."""
    start_time = datetime.now()
    query_id = f"q-{start_time.strftime('%Y%m%d-%H%M%S')}"
    
    logger.info(f"[{query_id}] Processing query: {prompt_input[:50]}...")
    
    try:
        # Check if RAG system is available
        if not rag_system:
            logger.error(f"[{query_id}] RAG system not available")
            return {"response": "‚ùå RAG system not available", "sources": []}
        
        # Initialize RAG system if needed
        logger.debug(f"[{query_id}] Initializing RAG system")
        if not rag_system.initialize():
            logger.error(f"[{query_id}] Failed to initialize RAG system")
            return {"response": "‚ùå Failed to initialize RAG system", "sources": []}
        
        # Get response from RAG system
        logger.info(f"[{query_id}] Sending query to RAG system")
        conversation_context = st.session_state.messages[:-1]
        logger.debug(f"[{query_id}] Context size: {len(conversation_context)} messages")
        
        response_data = rag_system.query(prompt_input, conversation_context)
        
        # Calculate response time
        response_time = (datetime.now() - start_time).total_seconds()
        logger.info(f"[{query_id}] Response generated in {response_time:.2f}s")
        
        if response_data.get("status") == "success":
            source_count = len(response_data.get("sources", []))
            logger.info(f"[{query_id}] Success - {source_count} sources found")
            
            # Log source details at debug level
            if source_count > 0:
                for i, source in enumerate(response_data.get("sources", [])[:3]):
                    logger.debug(f"[{query_id}] Source {i+1}: {source.get('title', 'Unknown')} - Score: {source.get('similarity_score', 0):.3f}")
            
            return {
                "response": response_data.get("response", "I couldn't generate a response."),
                "sources": response_data.get("sources", [])
            }
        else:
            error_msg = response_data.get('message', 'Something went wrong.')
            logger.warning(f"[{query_id}] Response error: {error_msg}")
            return {
                "response": f"‚ùå {error_msg}",
                "sources": []
            }
            
    except Exception as e:
        logger.exception(f"[{query_id}] Exception while generating response: {str(e)}")
        return {
            "response": f"‚ùå Sorry, I encountered an error: {str(e)}",
            "sources": []
        }

# 4. Accept prompt input
if prompt := st.chat_input("Ask about GitLab's handbook, policies, values, and culture...", key="main_input"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.session_state.query_count += 1
    with st.chat_message("user"):
        st.write(prompt)

# 5. Generate a new LLM response
try:
    logger.info("Checking if new response is needed")
    
    # Safety check for empty messages array
    if not st.session_state.messages:
        logger.warning("Messages array is empty, resetting")
        st.session_state.messages = [{"role": "assistant", "content": "How may I help you explore GitLab's handbook today?"}]
        st.rerun()
    
    # Check if last message is from user
    if st.session_state.messages[-1]["role"] != "assistant":
        user_message = st.session_state.messages[-1]["content"]
        logger.info(f"Generating response to: {user_message[:50]}...")
        
        with st.spinner("ü§î Thinking..."):
            try:
                response_data = generate_gitpulse_response(user_message)
                logger.info("Response generated successfully")
            except Exception as e:
                logger.exception("Error during response generation")
                response_data = {
                    "response": f"‚ö†Ô∏è I encountered an error while generating a response: {str(e)}",
                    "sources": []
                }
        
        # Store sources for sidebar display
        try:
            if response_data.get("sources"):
                logger.debug(f"Storing {len(response_data['sources'])} sources in session state")
                st.session_state.last_sources = response_data["sources"]
        except Exception as e:
            logger.exception("Error storing sources")
        
        # Add assistant response to session state
        try:
            message = {"role": "assistant", "content": response_data.get("response", "Error generating response")}
            logger.info(f"Adding assistant response ({len(message['content'])} chars)")
            st.session_state.messages.append(message)
            logger.info(f"Session now has {len(st.session_state.messages)} messages")
            st.rerun()
        except Exception as e:
            logger.exception("Error adding response to session state")
            st.error(f"Failed to process response: {str(e)}")
except Exception as e:
    logger.exception("CRITICAL ERROR in response generation flow")
    st.error("An unexpected error occurred. Please refresh the page and try again.")
    import traceback
    st.code(traceback.format_exc()) 